{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def london_url(page, year):\n",
    "    return f\"https://results.tcslondonmarathon.com/{year}/?page={page}&pid=list\"\n",
    "# get list of urls \n",
    "list_of_urls = []\n",
    "for year in range(2021, 2024):\n",
    "    for page in range(1, 60):\n",
    "        url = london_url(page, year)\n",
    "        list_of_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_s = []\n",
    "places_p = []\n",
    "names = []\n",
    "times = []\n",
    "cats = []\n",
    "divs = []\n",
    "years = []\n",
    "\n",
    "for url in list_of_urls:\n",
    "\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser') \n",
    "    tab_rows = soup.find_all('li', class_=\"list-active list-group-item row\")\n",
    "    year_curr = re.findall(\"(?<=.com/)(.*)(?=\\/\\?page)\", url)[0]\n",
    "    tab_rows2 = soup.find_all('li', class_=\"list-group-item row\")\n",
    "\n",
    "    for row in tab_rows:\n",
    "        place_s = row.find('div', class_=\"list-field type-place place-secondary hidden-xs numeric\")\n",
    "        places_s.append(place_s.text.strip())\n",
    "        place_p = row.find('div', class_=\"list-field type-place place-primary numeric\")\n",
    "        places_p.append(place_p.text.strip())\n",
    "        name = row.find('h4', class_=\"list-field type-fullname\")\n",
    "        names.append(name.text.strip())\n",
    "        time = row.find('div', class_=\"split list-field type-time\")\n",
    "        times.append(time.text.strip())\n",
    "        category = row.find('div', class_=\"list-field type-age_class\")\n",
    "        cats.append(category.text.strip())\n",
    "        divis = row.find('div', class_=\"list-field type-age_class\")\n",
    "        divs.append(divis.text.strip())\n",
    "        years.append(year_curr)\n",
    "    for row in tab_rows2:\n",
    "        place_s = row.find('div', class_=\"list-field type-place place-secondary hidden-xs numeric\")\n",
    "        places_s.append(place_s.text.strip())\n",
    "        place_p = row.find('div', class_=\"list-field type-place place-primary numeric\")\n",
    "        places_p.append(place_p.text.strip())\n",
    "        name = row.find('h4', class_=\"list-field type-fullname\")\n",
    "        names.append(name.text.strip())\n",
    "        time = row.find('div', class_=\"split list-field type-time\")\n",
    "        times.append(time.text.strip())\n",
    "        category = row.find('div', class_=\"list-field type-age_class\")\n",
    "        cats.append(category.text.strip())\n",
    "        divis = row.find('div', class_=\"list-field type-age_class\")\n",
    "        divs.append(divis.text.strip())\n",
    "        years.append(year_curr)\n",
    "london = pd.DataFrame(list(zip(names, places_s, places_p, times, cats, years)),\n",
    "               columns =['Name', 'Place S', 'Place P', 'Time', 'Division', 'Year']) \n",
    "london.to_csv(\"london_2021_2023.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_urls = []\n",
    "for page in range(1, 60):\n",
    "    url = london_url(page, 2019)\n",
    "    list_of_urls.append(url)\n",
    "\n",
    "places_s = []\n",
    "places_p = []\n",
    "names = []\n",
    "times = []\n",
    "cats = []\n",
    "divs = []\n",
    "years = []\n",
    "\n",
    "for url in list_of_urls:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser') \n",
    "    tab_rows = soup.find_all('li', class_=\"list-active list-group-item row\")\n",
    "    year_curr = re.findall(\"(?<=.com/)(.*)(?=\\/\\?page)\", url)[0]\n",
    "    tab_rows2 = soup.find_all('li', class_=\"list-group-item row\")\n",
    "\n",
    "\n",
    "    for row in tab_rows:\n",
    "        place_s = row.find('div', class_=\"list-field type-place place-secondary hidden-xs numeric\")\n",
    "        places_s.append(place_s.text.strip())\n",
    "        place_p = row.find('div', class_=\"list-field type-place place-primary numeric\")\n",
    "        places_p.append(place_p.text.strip())\n",
    "        name = row.find('h4', class_=\"list-field type-fullname\")\n",
    "        names.append(name.text.strip())\n",
    "        time = row.find('div', class_=\"split list-field type-time\")\n",
    "        times.append(time.text.strip())\n",
    "        category = row.find('div', class_=\"list-field type-age_class\")\n",
    "        cats.append(category.text.strip())\n",
    "        divis = row.find('div', class_=\"list-field type-age_class\")\n",
    "        divs.append(divis.text.strip())\n",
    "        years.append(year_curr)\n",
    "    for row in tab_rows2:\n",
    "        place_s = row.find('div', class_=\"list-field type-place place-secondary hidden-xs numeric\")\n",
    "        places_s.append(place_s.text.strip())\n",
    "        place_p = row.find('div', class_=\"list-field type-place place-primary numeric\")\n",
    "        places_p.append(place_p.text.strip())\n",
    "        name = row.find('h4', class_=\"list-field type-fullname\")\n",
    "        names.append(name.text.strip())\n",
    "        time = row.find('div', class_=\"split list-field type-time\")\n",
    "        times.append(time.text.strip())\n",
    "        category = row.find('div', class_=\"list-field type-age_class\")\n",
    "        cats.append(category.text.strip())\n",
    "        divis = row.find('div', class_=\"list-field type-age_class\")\n",
    "        divs.append(divis.text.strip())\n",
    "        years.append(year_curr)\n",
    "\n",
    "london = pd.DataFrame(list(zip(names, places_s, places_p, times, cats, years)),\n",
    "               columns =['Name', 'Place S', 'Place P', 'Time', 'Division', 'Year']) \n",
    "london.to_csv(\"london_2019.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_urls = []\n",
    "for page in range(1, 60):\n",
    "    url = london_url(page, 2018)\n",
    "    list_of_urls.append(url)\n",
    "\n",
    "data_all = pd.DataFrame() # initialize df \n",
    "\n",
    "for url in list_of_urls:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    year = re.findall(\"(?<=.com/)(.*)(?=\\/\\?page)\",url)\n",
    "    table = soup.find('table', class_='list-table')\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "        data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns)\n",
    "    yearcol = year * len(df)\n",
    "    df['year'] = yearcol \n",
    "\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv('london_2018.csv', index = False)\n",
    "\n",
    "list_of_urls = []\n",
    "for page in range(1, 60):\n",
    "    url = london_url(page, 2017)\n",
    "    list_of_urls.append(url)\n",
    "\n",
    "data_all = pd.DataFrame() # initialize df \n",
    "\n",
    "for url in list_of_urls:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    year = re.findall(\"(?<=.com/)(.*)(?=\\/\\?page)\",url)\n",
    "    table = soup.find('table', class_='list-table')\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "        data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns)\n",
    "    yearcol = year * len(df)\n",
    "    df['year'] = yearcol \n",
    "\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv('london_2017.csv', index = False)\n",
    "list_of_urls = []\n",
    "for page in range(1, 60):\n",
    "    url = london_url(page, 2016)\n",
    "    list_of_urls.append(url)\n",
    "\n",
    "data_all = pd.DataFrame() # initialize df \n",
    "\n",
    "for url in list_of_urls:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    year = re.findall(\"(?<=.com/)(.*)(?=\\/\\?page)\",url)\n",
    "    table = soup.find('table', class_='list-table')\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "        data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns)\n",
    "    yearcol = year * len(df)\n",
    "    df['year'] = yearcol \n",
    "\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "data_all.to_csv('london_2016.csv', index = False)\n",
    "list_of_urls = []\n",
    "for page in range(1, 60):\n",
    "    url = london_url(page, 2015)\n",
    "    list_of_urls.append(url)\n",
    "\n",
    "data_all = pd.DataFrame() # initialize df \n",
    "\n",
    "for url in list_of_urls:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    year = re.findall(\"(?<=.com/)(.*)(?=\\/\\?page)\",url)\n",
    "    table = soup.find('table', class_='list-table')\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "        data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns)\n",
    "    yearcol = year * len(df)\n",
    "    df['year'] = yearcol \n",
    "\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "data_all.to_csv('london_2015.csv', index = False)\n",
    "list_of_urls = []\n",
    "for page in range(1, 60):\n",
    "    url = london_url(page, 2014)\n",
    "    list_of_urls.append(url)\n",
    "\n",
    "data_all = pd.DataFrame() # initialize df \n",
    "\n",
    "for url in list_of_urls:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    year = re.findall(\"(?<=.com/)(.*)(?=\\/\\?page)\",url)\n",
    "    table = soup.find('table', class_='list-table')\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "        data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns)\n",
    "    yearcol = year * len(df)\n",
    "    df['year'] = yearcol \n",
    "\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv('london_2014.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
