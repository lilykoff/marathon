{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buttons_to_click(num):\n",
    "    # return f'/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/div/ul/li[{num}]/a'\n",
    "    return f'//*[@id=\"resultTable_paginate\"]/ul/li[{num}]/a'\n",
    "\n",
    "def buttons_to_click5(num):\n",
    "    return f'//*[@id=\"resultTable_paginate\"]/ul/li[5]/a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(1)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[4]/div/div/div[2]/div[2]/div[2]/div[1]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2021\")\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(2)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,300):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(2)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "data_all.to_csv(\"berlin_2021.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(1)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[3]/div/div/div[2]/div[2]/div[2]/div[1]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2022\")\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(2)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,600):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(2)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2022.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[3]/div/div/div[2]/div[2]/div[2]/div[2]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2019\")\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,600):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2019.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a10a204234f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuttons_to_click5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[4]/div/div/div[2]/div[2]/div[2]/div[2]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2019\")\n",
    "time.sleep(1)\n",
    "# add below to get women first \n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,2300):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2019_w.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[4]/div/div/div[2]/div[2]/div[2]/div[2]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2018\")\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,600):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2018.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[3]/div/div/div[2]/div[2]/div[2]/div[2]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2017\")\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,600):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2017.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=117.0.5938.92)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d39e809e1a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuttons_to_click5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mpage_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \"\"\"\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_PAGE_SOURCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=117.0.5938.92)\n"
     ]
    }
   ],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[4]/div/div/div[2]/div[2]/div[2]/div[2]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2016\")\n",
    "time.sleep(1)\n",
    "# add below to get women first \n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,3600):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2016_v2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[4]/div/div/div[2]/div[2]/div[2]/div[2]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2016\")\n",
    "time.sleep(1)\n",
    "# add below to get women first \n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,3600):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2016_w.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[4]/div/div/div[2]/div[2]/div[2]/div[2]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2016\")\n",
    "time.sleep(1)\n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,1000):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2016.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[3]/div/div/div[2]/div[2]/div[2]/div[2]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2015\")\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,600):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2015.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[3]/div/div/div[2]/div[2]/div[2]/div[2]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2014\")\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,600):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2014.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[4]/div/div/div[2]/div[2]/div[2]/div[2]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2017\")\n",
    "time.sleep(1)\n",
    "# add below to get women first \n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,2300):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2017_w.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[4]/div/div/div[2]/div[2]/div[2]/div[2]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2015\")\n",
    "time.sleep(1)\n",
    "# add below to get women first \n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,2300):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2015_w.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/lilykoffman/Downloads/chromedriver\")\n",
    "# Open the webpage\n",
    "url = \"https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, value=\"/html/body/div[4]/div/div/div[2]/div[2]/div[2]/div[2]/button/span\").click()\n",
    "time.sleep(1)\n",
    "year = Select(driver.find_element(By.ID, value=\"editions\"))\n",
    "year.select_by_value(\"2014\")\n",
    "time.sleep(1)\n",
    "# add below to get women first \n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "driver.find_element(By.XPATH, value = \"/html/body/main/section/div[2]/div/div/div/div/div[4]/div/div/div/table/thead/tr/th[7]\").click()\n",
    "time.sleep(1)\n",
    "sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "for num in range(2,6):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "for num in range(6,2300):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, value=buttons_to_click5(num)).click()\n",
    "    time.sleep(1)\n",
    "    sources.append(driver.page_source)\n",
    "time.sleep(1)\n",
    "driver.quit()\n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser') \n",
    "    table = soup.find('table', id=\"resultTable\")\n",
    "    columns = []\n",
    "    data = []\n",
    "\n",
    "# Extract column names from the table header (th elements)\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for header_cell in header_row.find_all('th'):\n",
    "        columns.append(header_cell.text.strip())\n",
    "\n",
    "# Extract data rows from the table body (td elements)\n",
    "    body = table.find('tbody')\n",
    "    for row in body.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "            data.append(row_data)\n",
    "    df = pd.DataFrame(data, columns = columns).drop_duplicates()\n",
    "    data_all = data_all.append(df, ignore_index = True)\n",
    "\n",
    "data_all.to_csv(\"berlin_2014_w.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
